{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e91415-b04a-41d5-9fd8-9f6b99152035",
   "metadata": {},
   "source": [
    "# Data Mining Homework 1\n",
    "\n",
    "陳奕君 111062610\n",
    "\n",
    "This file contains the fourth part of HW1 (10%). The answer includes some code from `DM2022-Lab1-Master.ipynb` and `1_Frist_part.ipynb`.\n",
    "\n",
    "**Description**\n",
    "\n",
    "In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f197389-4f95-43a2-b840-78c1a5211758",
   "metadata": {},
   "source": [
    "**My Answer**\n",
    "\n",
    "1. Calculate the feature \"unigrams\" is slow and unmeaningful. After we calculate this feature, we didn't use it at all. It's more efficient to use sklearn `CountVectorizer` because it runs faster and can give us the data that we want, e.g. `X_counts`.\n",
    "2. It's not so meaningful to use heatmap as a visualization of occurance of terms in documents. Since the term-document matrix is a sparse 0-1 matrix, we don't need the color change from 0 to 1. I suggest using a scatter plot, which is a better visualization for term-document matrix. If there is a point, that means 1, and others are 0. In this way we can include all the documents and terms inside our graph without making too messy.\n",
    "3. The first way we calculate `term_frequencies` is very inefficient.\n",
    "    ```python\n",
    "    term_frequencies = []\n",
    "    for j in range(0,X_counts.shape[1]):\n",
    "        term_frequencies.append(sum(X_counts[:,j].toarray()))\n",
    "        \n",
    "    ```\n",
    "    \n",
    "    \n",
    "    This code needs 87.8983359336853 seconds to finish processing. I calculate it by time library of Python. It's better to use the second way to calculate it, which only needs about 0.01 second.\n",
    "    \n",
    "    ```python\n",
    "    term_frequencies = np.asarray(X_counts.sum(axis=0))\n",
    "    \n",
    "    ```\n",
    "    As for the reason why the second way is faster, I think it's because `pandas` libraray has done some optimization on summing the columns and rows in DataFrame. Running through a loop to append elements in list is apparant to be slower than in-library function.\n",
    "4. Showing 300 features in a graph in that way is unmeaningful. Since the image is too small and the information is too much, it's hard to see clearly about the value and the featuer name. I suggest it's better to use my funciton `vis_term`. It can customize how many terms you want to visualize and wherer the term id starts. In this way, the user can visualize the term frequencies in the number they want.\n",
    "\n",
    "    ```python\n",
    "    # Default we visualize first 100 terms\n",
    "    def vis_term(num=100, start=0):\n",
    "        tf_temp = term_frequencies[start:start+num]\n",
    "        fn_temp = count_vect.get_feature_names_out()[start:start+num]\n",
    "        fig = px.bar(x=fn_temp, y=tf_temp)\n",
    "        fig.show()\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
